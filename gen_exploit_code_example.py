from model import ExploitGen
from transformers import RobertaConfig, RobertaTokenizerFast
from utils import preprocess_single_data

# Load the trained model
config = RobertaConfig.from_pretrained('./fg_codebert_model')
model = ExploitGen.from_pretrained('./fg_codebert_model', config=config)
tokenizer = RobertaTokenizerFast.from_pretrained('./fg_codebert_model')

# Generate code from natural language
nl_description = "set the variable z to bitwise not x"
processed = preprocess_single_data(nl_description, "dummy_code")

# Tokenize and generate
raw_encodings = tokenizer(nl_description, return_tensors='pt')
temp_encodings = tokenizer(processed['template_nl'], return_tensors='pt')

# Generate code
with torch.no_grad():
    generated_ids = model.generate(
        input_ids=raw_encodings['input_ids'],
        attention_mask=raw_encodings['attention_mask'],
        max_length=64,
        num_beams=10,
        early_stopping=True,
        num_return_sequences=1
    )

# Decode and post-process
template_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
final_code = template_code
for placeholder, original_token in processed['slot_map'].items():
    final_code = final_code.replace(placeholder, original_token)

print(f"Generated Code: {final_code}")