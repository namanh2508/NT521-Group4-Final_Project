

# ExploitGen: Template-augmented Exploit Code Generation based on CodeBERT

This project implements the ExploitGen model described in the paper "ExploitGen: Template-augmented exploit code generation based on CodeBERT" published in The Journal of Systems & Software. The model generates exploit code from natural language descriptions using a template-augmented approach based on CodeBERT.

## Table of Contents
- [Project Overview](#project-overview)
- [Directory Structure](#directory-structure)
- [Installation](#installation)
- [Data Preparation](#data-preparation)
- [Training Process](#training-process)
  - [Domain-Adaptive Pre-training (DAPT)](#domain-adaptive-pre-training-dapt)
  - [Task-Adaptive Pre-training (TAPT)](#task-adaptive-pre-training-tapt)
  - [ExploitGen Model Training](#exploitgen-model-training)
- [Evaluation](#evaluation)
- [Usage Examples](#usage-examples)
- [Citation](#citation)

## Project Overview

ExploitGen addresses the challenging task of automatically generating exploit code from natural language descriptions. The key innovations include:

- **Template Parser**: A rule-based component that extracts domain-specific tokens and replaces them with placeholders
- **Dual-Encoder Architecture**: Uses two encoders to process both raw and template-augmented natural language
- **Semantic Attention Layer**: Dynamically combines information from different layers of the encoders
- **Fusion Layer**: Effectively integrates template information with raw semantic information
- **Decoder**: Generates the final code sequence using Transformer's decoder architecture
- **Adaptive Pre-training**: Combines Domain-Adaptive Pre-training (DAPT) and Task-Adaptive Pre-training (TAPT) to create FG-CodeBERT
  
## Directory Structure

A detailed breakdown of the project's files and directories to help you navigate the codebase.

ExploitGen/

â”œâ”€â”€ config.py # âš™ï¸ Central configuration file for paths, hyperparameters, and model settings.

â”œâ”€â”€ model.py # ğŸ§  PyTorch model definitions (ExploitGen, SemanticAttention, FusionLayer, etc.).

â”œâ”€â”€ utils.py # ğŸ› ï¸ Utility functions for data loading, tokenization, and Template Parser.

â”œâ”€â”€ dapt_training.py # ğŸš€ Domain-Adaptive Pre-training (DAPT) on the SPoC dataset.

â”œâ”€â”€ tapt_training.py # ğŸš€ Task-Adaptive Pre-training (TAPT) on exploit code datasets.

â”œâ”€â”€ train.py # ğŸ‹ï¸ Final training script using the two-stage strategy + FG-CodeBERT.

â”œâ”€â”€ evaluate.py # ğŸ“Š Inference & evaluation on the test set.

â”œâ”€â”€ gen_exploit_code_example.py # After running and training this model successfully, use this code to test

â”œâ”€â”€ requirements.txt # ğŸ“¦ Python dependencies.

â”œâ”€â”€ README.md # ğŸ“– Project documentation.


â”œâ”€â”€ data/ # ğŸ“‚ All datasets.

â”‚ â”œâ”€â”€ spoc/

â”‚ â”‚ â””â”€â”€ train/

â”‚ â”‚ â””â”€â”€â”€â”€ spoc-train.tsv # SPoC dataset for DAPT (manual download).

â”‚ â”œâ”€â”€ python/

â”‚ â”‚ â”œâ”€â”€â”€â”€ train.csv # Python exploit code â€” training set.

â”‚ â”‚ â”œâ”€â”€â”€â”€ dev.csv # Python exploit code â€” dev/validation set.

â”‚ â”‚ â””â”€â”€â”€â”€ test.csv # Python exploit code â€” test set.

â”‚ â””â”€â”€ assembly/

â”‚ â”œâ”€â”€â”€â”€ train.csv # Assembly exploit code â€” training set.

â”‚ â”œâ”€â”€â”€â”€ dev.csv # Assembly exploit code â€” dev/validation set.

â”‚ â””â”€â”€â”€â”€ test.csv # Assembly exploit code â€” test set.


â”œâ”€â”€ codeBERT-dapt/ # ğŸ“‚ Output from DAPT ( created by dapt_training.py).


â”œâ”€â”€ fg_codebert_model/ # ğŸ“‚ Final FG-CodeBERT from TAPT ( created by tapt_training.py).


â”œâ”€â”€ checkpoint-epoch-{epoch_number}/ # ğŸ“‚ Saved ExploitGen checkpoints from each epoch. ( created by train.py )

â””â”€â”€ logs/ # ğŸ“‚ Training logs. ( created by dapt_training.py and tapt_training.py )

â”œâ”€â”€â”€â”€ dapt/ # TensorBoard logs for DAPT.

â””â”€â”€â”€â”€ tapt/ # TensorBoard logs for TAPT.

***Note***: Directories like codeBERT-dapt/, fg_codebert_model/, checkpoint-epoch-{}/, and logs/ are automatically created during training.

## Installation

### Prerequisites
- Python 3.7 or higher
- PyTorch 1.8.0 or higher
- CUDA-compatible GPU (recommended for training)

### Setup Steps

1. Clone the repository:
```bash
git clone https://github.com/NTDXYG/ExploitGen.git
cd ExploitGen
```

2. Create a virtual environment (recommended):
```bash
python -m venv venv
source venv\Scripts\activate  # On Windows
source venv/bin/activate      # On Linux
```

3. Install the required packages:
```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126  # Thay Ä‘á»•i phiÃªn báº£n CUDA, Pytorch á»Ÿ Ä‘Ã¢y ( https://pytorch.org/ )
pip install -r requirements.txt
```

4. Download spaCy model:
```bash
python -m spacy download en_core_web_sm
```

## Data Preparation

1. Create the data directory structure:
```bash
mkdir -p data/spoc
mkdir -p data/python
mkdir -p data/assembly
```

2. Download and prepare the SPoC dataset for DAPT:
   - Download the SPoC dataset from the official source ( https://github.com/sumith1896/spoc.git )
   - Extract and place `spoc-train.tsv` in the `data/spoc/` directory

3. Prepare the exploit code datasets:
   - Download the Python and Assembly exploit datasets from ( https://github.com/NTDXYG/ExploitGen ) 
   - Place the processed CSV files from above Python and Assembly folders in `data/python/` and `data/assembly/` directories

   The CSV files should have the following columns:
   - `raw_nl`: Original natural language description
   - `temp_nl`: Template-augmented natural language
   - `raw_code`: Original exploit code
   - `temp_code`: Template-augmented exploit code

## Training Process

The training process consists of three main stages:

### Domain-Adaptive Pre-training (DAPT)

This stage adapts the original CodeBERT model to the domain of competitive programming using the SPoC dataset.

```bash
python dapt_training.py
```

The pre-trained model will be saved to `./codeBERT-dapt/` directory.

### Task-Adaptive Pre-training (TAPT)

This stage further adapts the model to the specific task of exploit code generation.

```bash
python tapt_training.py
```

The fine-tuned FG-CodeBERT model will be saved to `./fg_codebert_model/` directory.

### ExploitGen Model Training

This stage trains the complete ExploitGen model using the two-stage training strategy.

```bash
python train.py
```

The trained model checkpoints will be saved to `./checkpoint-epoch-{epoch_number}/` directories.

## Evaluation

To evaluate the trained model:

```bash
python evaluate.py
```

This will:
1. Generate code for a sample natural language description
2. Evaluate the model on the test set (commented out by default)
3. Calculate BLEU-4, ROUGE-W, and Exact Match metrics

## Usage Examples
```bash
python gen_exploit_code_example.py
```

## Citation

If you use this code in your research, please cite our paper:

```bibtex
@article{yang2023exploitgen,
  title={ExploitGen: Template-augmented exploit code generation based on CodeBERT},
  author={Yang, Guang and Zhou, Yu and Chen, Xiang and Zhang, Xiangyu and Han, Tingting and Chen, Taolue},
  journal={The Journal of Systems \& Software},
  volume={197},
  pages={111577},
  year={2023},
  publisher={Elsevier}
}
```

## License

This project is licensed under UIT - NT521.Q12.ANTT - Group 08 

## Contact

[Email: 23520075@gm.uit.edu.vn](mailto:23520075@gm.uit.edu.vn)

[Email: 23520281@gm.uit.edu.vn](mailto:23520281@gm.uit.edu.vn)

[Email: 23521610@gm.uit.edu.vn](mailto:23521610@gm.uit.edu.vn)

[Email: 23521260@gm.uit.edu.vn](mailto:23521260@gm.uit.edu.vn)

## Acknowledgments

We thank the original authors of the paper and the contributors to the CodeBERT project. We also acknowledge the providers of the datasets used in this project.
